{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tahapan-tahapan :\n",
    "    1. preproses data/feuture Engineering\n",
    "    2. train data\n",
    "    3. test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "preproses data/feuture Engineering:\n",
    "\n",
    "A. Clean data \n",
    "    1. menghilangkan karakter-karakter kata yang dianggap tdk penting dan Punctuation (Special character cleaning)\n",
    "    2. mengubah kata-kata menjadi lowercase (Upcase/downcase)\n",
    "    3. menghilangkan kata-kata yg tidak penting (stopword)\n",
    "    4. mengembalikan kata-kata kedalam bentuk asli nya nya (Stemming/Lemmatization)\n",
    "    5. mengkoreksi beberapa kata (typo correction)\n",
    "    \n",
    "B. Text representation\n",
    "    1. Word Count Vectors : menghitung setiap jumlah kata/corpus yg muncul dalam kumpulan data tersebut\n",
    "    2. TF–IDF Vectors : menghitung jarak kata antar kata dengan rumus yg telah di tentukan tanpa memperhitungkan urutan.\n",
    "    3. Word Embeddings : setiap kata yang muncul akan dihitung kedekatan nya dengan kata yg disekitar nya\n",
    "    4. Text based or NLP based features : membuat secara manual fitur2 yang akan digunakan, misal menggabungkan antar perhitungan jumlah kata, jarak antar kata, kedudukan sebuah kata, atau pun sifat dari kata tersebut (post tagger)\n",
    "    5. Topic Models : dalam setiap kumpulan data di lihat secara probability kata yang paling dominan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from sklearn.datasets import load_files\n",
    "#nltk.download('stopwords')\n",
    "import pickle\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_data = load_files(r\"E:\\txt_sentoken\")\n",
    "X, y = movie_data.data, movie_data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = []\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stemmer = WordNetLemmatizer()\n",
    "\n",
    "for sen in range(0, len(X)):\n",
    "    # Remove all the special characters\n",
    "    document = re.sub(r'\\W', ' ', str(X[sen]))\n",
    "    \n",
    "    # remove all single characters\n",
    "    document = re.sub(r'\\s+[a-zA-Z]\\s+', ' ', document)\n",
    "    \n",
    "    # Remove single characters from the start\n",
    "    document = re.sub(r'\\^[a-zA-Z]\\s+', ' ', document) \n",
    "    \n",
    "    # Substituting multiple spaces with single space\n",
    "    document = re.sub(r'\\s+', ' ', document, flags=re.I)\n",
    "    \n",
    "    # Removing prefixed 'b'\n",
    "    document = re.sub(r'^b\\s+', '', document)\n",
    "    \n",
    "    # Converting to Lowercase\n",
    "    document = document.lower()\n",
    "    \n",
    "    # Lemmatization\n",
    "    document = document.split()\n",
    "\n",
    "    document = [stemmer.lemmatize(word) for word in document]\n",
    "    document = ' '.join(document)\n",
    "    \n",
    "    documents.append(document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'arnold schwarzenegger ha been an icon for action enthusiast since the late 80 but lately his film have been very sloppy and the one liner are getting worse nit hard seeing arnold a mr freeze in batman and robin especially when he say ton of ice joke but hey he got 15 million what it matter to him nonce again arnold ha signed to do another expensive blockbuster that can compare with the like of the terminator series true lie and even eraser nin this so called dark thriller the devil gabriel byrne ha come upon earth to impregnate woman robin tunney which happens every 1000 year and basically destroy the world but apparently god ha chosen one man and that one man is jericho cane arnold himself nwith the help of trusty sidekick kevin pollack they will stop at nothing to let the devil take over the world nparts of this are actually so absurd that they would fit right in with dogma nyes the film is that weak but it better than the other blockbuster right now sleepy hollow but it make the world is not enough look like 4 star film nanyway this definitely doesn seem like an arnold movie nit just wasn the type of film you can see him doing nsure he gave u few chuckle with his well known one liner but he seemed confused a to where his character and the film wa going nit understandable especially when the ending had to be changed according to some source naside form that he still walked through it much like he ha in the past few film ni sorry to say this arnold but maybe these are the end of your action day nspeaking of action where wa it in this film nthere wa hardly any explosion or fight nthe devil made few place explode but arnold wasn kicking some devil butt nthe ending wa changed to make it more spiritual which undoubtedly ruined the film ni wa at least hoping for cool ending if nothing else occurred but once again wa let down ni also don know why the film took so long and cost so much nthere wa really no super affect at all unless you consider an invisible devil who wa in it for 5 minute top worth the overpriced budget nthe budget should have gone into better script where at least audience could be somewhat entertained instead of facing boredom nit pitiful to see how script like these get bought and made into movie ndo they even read these thing anymore nit sure doesn seem like it nthankfully gabriel performance gave some light to this poor film nwhen he walk down the street searching for robin tunney you can help but feel that he looked like devil nthe guy is creepy looking anyway nwhen it all over you re just glad it the end of the movie ndon bother to see this if you re expecting solid action flick because it neither solid nor doe it have action nit just another movie that we are suckered in to seeing due to strategic marketing campaign nsave your money and see the world is not enough for an entertaining experience'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Text to Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finding TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(max_features=1500, min_df=5, max_df=0.7, stop_words=stopwords.words('english'))\n",
    "X = vectorizer.fit_transform(documents).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [2, 0, 0, ..., 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 1, ..., 1, 0, 0])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training and Testing Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 1, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 7, 0],\n",
       "       [0, 0, 1, ..., 0, 1, 0]], dtype=int64)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training Text Classification Model and Predicting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(n_estimators=1000, random_state=0)"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=1000, random_state=0)\n",
    "classifier.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 1, 0, 0,\n",
       "       1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0,\n",
       "       0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 1,\n",
       "       0, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0,\n",
       "       1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1,\n",
       "       1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0,\n",
       "       0, 0, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0,\n",
       "       0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0, 1,\n",
       "       0, 1, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 0, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 1, 1, 0,\n",
       "       1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1,\n",
       "       1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "       0, 0, 1, 0, 1, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 1,\n",
       "       0, 0, 1, 0])"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[173  35]\n",
      " [ 20 172]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.90      0.83      0.86       208\n",
      "           1       0.83      0.90      0.86       192\n",
      "\n",
      "    accuracy                           0.86       400\n",
      "   macro avg       0.86      0.86      0.86       400\n",
      "weighted avg       0.86      0.86      0.86       400\n",
      "\n",
      "0.8625\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# data bahasa indo "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>positive</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>positive</td>\n",
       "      <td>keren mvnya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>positive</td>\n",
       "      <td>untuk seseorang yang sedang bersamaku terimaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>positive</td>\n",
       "      <td>luar biasa mental illness nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>positive</td>\n",
       "      <td>sibuk jatuh cinta sendirian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                           sentence\n",
       "5995  positive                                              bagus\n",
       "5996  positive                                        keren mvnya\n",
       "5997  positive  untuk seseorang yang sedang bersamaku terimaka...\n",
       "5998  positive                      luar biasa mental illness nya\n",
       "5999  positive                        sibuk jatuh cinta sendirian"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"data.csv\", delimiter=\";\", encoding = 'iso-8859-1')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_new = list(data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].factorize()[0]\n",
    "from io import StringIO\n",
    "kategori_id_data = data[['label', 'label_id']].drop_duplicates().sort_values('label_id')\n",
    "kategori_to_id = dict(kategori_id_data.values)\n",
    "id_to_kategori = dict(kategori_id_data[['label_id', 'label']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_kategori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "      <th>label_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>negative</td>\n",
       "      <td>belum bisa membaca flazz di kartu paspor domestik</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>negative</td>\n",
       "      <td>aplikasi jelek programmer memprogram aplikasi ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>negative</td>\n",
       "      <td>setelah di update belum bisa login</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>negative</td>\n",
       "      <td>koneksi terputus belum bisa transaksi</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>negative</td>\n",
       "      <td>aplikasi susah login</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>positive</td>\n",
       "      <td>bagus</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>positive</td>\n",
       "      <td>keren mvnya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>positive</td>\n",
       "      <td>untuk seseorang yang sedang bersamaku terimaka...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>positive</td>\n",
       "      <td>luar biasa mental illness nya</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>positive</td>\n",
       "      <td>sibuk jatuh cinta sendirian</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6000 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                           sentence  label_id\n",
       "0     negative  belum bisa membaca flazz di kartu paspor domestik         0\n",
       "1     negative  aplikasi jelek programmer memprogram aplikasi ...         0\n",
       "2     negative                 setelah di update belum bisa login         0\n",
       "3     negative              koneksi terputus belum bisa transaksi         0\n",
       "4     negative                               aplikasi susah login         0\n",
       "...        ...                                                ...       ...\n",
       "5995  positive                                              bagus         2\n",
       "5996  positive                                        keren mvnya         2\n",
       "5997  positive  untuk seseorang yang sedang bersamaku terimaka...         2\n",
       "5998  positive                      luar biasa mental illness nya         2\n",
       "5999  positive                        sibuk jatuh cinta sendirian         2\n",
       "\n",
       "[6000 rows x 3 columns]"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "y_list = list(data['label_id'])\n",
    "y_new = np.array(y_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 2, 2, 2])"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x_new, y_new, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect = CountVectorizer()\n",
    "X_train_counts = count_vect.fit_transform(X_train)\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = MultinomialNB().fit(X_train_tfidf, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = count_vect.transform(X_test)\n",
    "X_coba = tfidf_transformer.fit_transform(a).toarray()\n",
    "\n",
    "y_pred = clf.predict(X_coba)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[380  49  65]\n",
      " [ 70 316 105]\n",
      " [ 49  49 417]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.77      0.77       494\n",
      "           1       0.76      0.64      0.70       491\n",
      "           2       0.71      0.81      0.76       515\n",
      "\n",
      "    accuracy                           0.74      1500\n",
      "   macro avg       0.75      0.74      0.74      1500\n",
      "weighted avg       0.74      0.74      0.74      1500\n",
      "\n",
      "0.742\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))\n",
    "print(accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =['hari ini sangat indah',\n",
    "       'kok kamu jelek bgt sih',\n",
    "       'ada berita duka nih hari ini',\n",
    "       'bagaimana kabar hari ini?',\n",
    "       'semua baik-baik2 saja kan',\n",
    "      'bagaimana dengan perasaan kamu hari ini?',\n",
    "      'kabar jogja hari ini',\n",
    "      'kapan corona nya pergi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'teks': 'hari ini sangat indah', 'label': 'positive'}\n",
      "{'teks': 'kok kamu jelek bgt sih', 'label': 'negative'}\n",
      "{'teks': 'ada berita duka nih hari ini', 'label': 'negative'}\n",
      "{'teks': 'bagaimana kabar hari ini?', 'label': 'neutral'}\n",
      "{'teks': 'semua baik-baik2 saja kan', 'label': 'positive'}\n",
      "{'teks': 'bagaimana dengan perasaan kamu hari ini?', 'label': 'positive'}\n",
      "{'teks': 'kabar jogja hari ini', 'label': 'positive'}\n",
      "{'teks': 'kapan corona nya pergi', 'label': 'neutral'}\n"
     ]
    }
   ],
   "source": [
    "for i in test :\n",
    "    a = count_vect.transform([i])\n",
    "    X_coba = tfidf_transformer.fit_transform(a).toarray()\n",
    "    y_pred = clf.predict(X_coba)\n",
    "    if y_pred[0]== 0 :\n",
    "        label = 'negative'\n",
    "    elif y_pred[0]== 1 :\n",
    "        label = 'neutral'\n",
    "    else :\n",
    "        label = 'positive'\n",
    "    print ({\"teks\": i , \"label\" : label})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "from keras import initializers\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Dropout, LSTM\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils.np_utils import to_categorical\n",
    "#import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "#import seaborn as sns\n",
    "import xlsxwriter\n",
    "from keras.models import load_model\n",
    "from keras.layers import Embedding, Conv1D, GlobalMaxPooling1D, Dense, Dropout, Flatten, MaxPooling1D, Input, Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>sentence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5995</th>\n",
       "      <td>positive</td>\n",
       "      <td>bagus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5996</th>\n",
       "      <td>positive</td>\n",
       "      <td>keren mvnya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5997</th>\n",
       "      <td>positive</td>\n",
       "      <td>untuk seseorang yang sedang bersamaku terimaka...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5998</th>\n",
       "      <td>positive</td>\n",
       "      <td>luar biasa mental illness nya</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5999</th>\n",
       "      <td>positive</td>\n",
       "      <td>sibuk jatuh cinta sendirian</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         label                                           sentence\n",
       "5995  positive                                              bagus\n",
       "5996  positive                                        keren mvnya\n",
       "5997  positive  untuk seseorang yang sedang bersamaku terimaka...\n",
       "5998  positive                      luar biasa mental illness nya\n",
       "5999  positive                        sibuk jatuh cinta sendirian"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"data.csv\", delimiter=\";\", encoding = 'iso-8859-1')\n",
    "data.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['label_id'] = data['label'].factorize()[0]\n",
    "from io import StringIO\n",
    "kategori_id_data = data[['label', 'label_id']].drop_duplicates().sort_values('label_id')\n",
    "kategori_to_id = dict(kategori_id_data.values)\n",
    "id_to_kategori = dict(kategori_id_data[['label_id', 'label']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'negative', 1: 'neutral', 2: 'positive'}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id_to_kategori"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "leng = kategori_id_data.label_id.values\n",
    "leng = len (leng)\n",
    "leng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pembentukan Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sumber model : https://github.com/diegoschapira/CNN-Text-Classifier-using-Keras/blob/master/models.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(X, Y):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim = vocab, output_dim = 128, input_length = max_sentence_length, embeddings_initializer = initializer))\n",
    "    model.add(Conv1D(128, 7, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(256, 5, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Conv1D(512, 3, activation='relu',padding='same'))\n",
    "    model.add(MaxPooling1D())\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(leng, activation='softmax'))\n",
    "    model.compile(optimizer='adam',\n",
    "                  loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluation(model, X, Y):\n",
    "    Y_pred = model.predict(X)\n",
    "    Y_pred_class = np.argmax(Y_pred, axis=1)\n",
    "    Y_act = Y\n",
    "    \n",
    "    acc = accuracy_score(Y_act, Y_pred_class)\n",
    "    \n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = data['sentence'].values\n",
    "label = data['label_id'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "#text1 = str(text1)\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text1)\n",
    "\n",
    "vocab = max([len(tokenizer.word_index)]) + 1 # kamus kata \n",
    "max_sentence_length = 100 # panjang kalimat\n",
    "batch_size = 32 # penentuan jumlah sampel yang ditraining pada tiap epoch\n",
    "num_epochs = 10 # banyak iterasi pada saat training model \n",
    "initializer = initializers.RandomUniform(minval=-0.05, maxval=0.05, seed=2) # mengatur angka random weigth\n",
    "embedding_dim = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Train\n",
    "X = tokenizer.texts_to_sequences(text1)\n",
    "X = pad_sequences(X, maxlen=max_sentence_length)\n",
    "Y = to_categorical(label, num_classes = leng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 100, 128)          1369728   \n",
      "_________________________________________________________________\n",
      "conv1d_1 (Conv1D)            (None, 100, 128)          114816    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1 (None, 50, 128)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_2 (Conv1D)            (None, 50, 256)           164096    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1 (None, 25, 256)           0         \n",
      "_________________________________________________________________\n",
      "conv1d_3 (Conv1D)            (None, 25, 512)           393728    \n",
      "_________________________________________________________________\n",
      "max_pooling1d_3 (MaxPooling1 (None, 12, 512)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6144)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               786560    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 387       \n",
      "=================================================================\n",
      "Total params: 2,829,315\n",
      "Trainable params: 2,829,315\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = get_model(X, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\nilam titip\\envs\\env_dev\\lib\\site-packages\\tensorflow_core\\python\\framework\\indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "6000/6000 [==============================] - 140s 23ms/step - loss: 1.0146 - accuracy: 0.4660\n",
      "Epoch 2/10\n",
      "6000/6000 [==============================] - 132s 22ms/step - loss: 0.6828 - accuracy: 0.7283\n",
      "Epoch 3/10\n",
      "6000/6000 [==============================] - 122s 20ms/step - loss: 0.3182 - accuracy: 0.8880\n",
      "Epoch 4/10\n",
      "6000/6000 [==============================] - 164s 27ms/step - loss: 0.1369 - accuracy: 0.9603\n",
      "Epoch 5/10\n",
      "6000/6000 [==============================] - 165s 28ms/step - loss: 0.0659 - accuracy: 0.9798\n",
      "Epoch 6/10\n",
      "6000/6000 [==============================] - 157s 26ms/step - loss: 0.0511 - accuracy: 0.9867\n",
      "Epoch 7/10\n",
      "6000/6000 [==============================] - 154s 26ms/step - loss: 0.0460 - accuracy: 0.9892\n",
      "Epoch 8/10\n",
      "6000/6000 [==============================] - 137s 23ms/step - loss: 0.0373 - accuracy: 0.9890\n",
      "Epoch 9/10\n",
      "6000/6000 [==============================] - 143s 24ms/step - loss: 0.0234 - accuracy: 0.9923\n",
      "Epoch 10/10\n",
      "6000/6000 [==============================] - 177s 30ms/step - loss: 0.0198 - accuracy: 0.9930\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x17f8af28548>"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# training model\n",
    "model.fit(X, Y, batch_size=batch_size, epochs=num_epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Akurasi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 99.45 %\n"
     ]
    }
   ],
   "source": [
    "acc = evaluation(model, X, label)\n",
    "print(\"Accuracy: %.2f\" % (acc*100), \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Menyimpan model dan memanggil model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('model_Deep_CNN.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model('model_Deep_CNN.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Data yang baru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test =['hari ini sangat indah',\n",
    "       'kok kamu jelek bgt sih',\n",
    "       'ada berita duka nih hari ini',\n",
    "       'bagaimana kabar hari ini?',\n",
    "       'semua baik-baik2 saja kan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = tokenizer.texts_to_sequences(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = pad_sequences(text1, maxlen=max_sentence_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_label = model.predict(text1)\n",
    "label1 = np.argmax(pred_label, axis = 1) \n",
    "ynew = model.predict_proba(text1)\n",
    "prob = ynew.max(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label =[]\n",
    "kal = []\n",
    "probab = []\n",
    "for unlabeled, label1, prob in zip(test, label1, prob):\n",
    "    print('\"{}\"'.format(unlabeled))\n",
    "    u = unlabeled\n",
    "    l = id_to_kategori[label1]\n",
    "    label.append(l)\n",
    "    kal.append(u)\n",
    "    probab.append(prob)\n",
    "    \n",
    "    print(\"  - Prediksi: '{}'\".format(id_to_kategori[label1]))\n",
    "    print(\"  - Probab: '{}'\".format(prob))\n",
    "    print(\"\")\n",
    "\n",
    "#print (label)\n",
    "#print (kal)\n",
    "#hasil =pd.DataFrame({\"Descript\" : kal, \"Category\" : label, \"probab\" : probab})\n",
    "#print (hasil)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
